{"cells":[{"cell_type":"markdown","id":"445a1170","metadata":{"id":"445a1170"},"source":["#    Notebook para procesar archivos de voz v0080101"]},{"cell_type":"code","source":["#Para descargar videos de Youtube\n","!pip install pytube\n","\n","#Para procesamiento de audio, convertir formatos de archivos de audio en otros formatos, convertir tasas de muestreo, para aplicar efectos de sonido, reproducir y grabar archivos de audio.\n","!apt -qq install -y sox\n","!apt -qq install -y sox libsox-fmt-mp3\n","!pip install sox\n","!git clone https://github.com/rabitt/pysox.git\n","!cd /content/pysox\n","!python /content/pysox/setup.py install\n","!pip install git+https://github.com/rabitt/pysox.git\n","\n","#convertidor de audio y video. Se usa para convertir WAV a MP3 y viceversa.\n","!pip3 install ffmpeg\n","!apt -qq install -y ffmpeg\n","\n","#Para reproducir, dividir, integrar o editar los archivos de audio únicamente con extensión .wav\n","!pip3 install pydub\n","\n","\n","#Para extraer datos de una hoja de cálculo, solo para archivos de extensión .xls\n","!pip3 install xlrd\n","!pip3 install --upgrade xlrd\n","\n","!pip install xlrd\n","!pip3 install xlrd\n","!pip install --upgrade --force-reinstall xlrd\n","\n","#Para manejar y analizar estructuras de datos.\n","!pip3 install pandas\n","!pip3 install --upgrade pandas\n"],"metadata":{"id":"0sglyiKX8LFD"},"id":"0sglyiKX8LFD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import shutil\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","\n","from IPython.display import display, Audio\n","from pathlib import Path\n","from scipy import stats\n","from scipy.io import wavfile\n","from sklearn.metrics import confusion_matrix\n","from tensorflow import keras\n","from google.colab import drive\n","drive.mount('/content/drive/')\n"],"metadata":{"id":"uv-F0heAvgjn"},"id":"uv-F0heAvgjn","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b4728317","metadata":{"id":"b4728317"},"outputs":[],"source":["# Variables de sesión\n","#DATASET_ROOT = \"/home/jmmiguez/proyectoAudio/audio/\"\n","DATASET_ROOT = '/content/drive/MyDrive/proyectoAudio/audio'\n","\n","# Las carpetas en las cuales voy a poner los ejemplos de audio y los ejemplos de ruidos\n","AUDIO_SUBFOLDER = \"audio\"\n","NOISE_SUBFOLDER = \"noise\"\n","DATASET_AUDIO_PATH = os.path.join(DATASET_ROOT, AUDIO_SUBFOLDER)\n","DATASET_NOISE_PATH = os.path.join(DATASET_ROOT, NOISE_SUBFOLDER)\n","\n","print (\"DATASET_ROOT {}\".format(DATASET_ROOT))\n","print (\"DATASET_AUDIO_PATH {}\".format(DATASET_AUDIO_PATH))\n","print (\"DATASET_NOISE_PATH {}\".format(DATASET_NOISE_PATH))\n","\n","# Porcentaje de muestras que voy a usar para validación\n","VALID_SPLIT = 0.2\n","\n","# Semilla que voy a usar para mezclar los datos con el ruido\n","SHUFFLE_SEED = 43\n","\n","# La tasa de muestreo a usar es única para todas las muestras de audio.\n","# Se vuelve a muestrear todo el ruido a esta frecuencia de muestreo.\n","# Este también será el tamaño de salida de las muestras de las señales de audio.\n","# (ya que todas las muestras son de 1 segundo de duración)\n","SAMPLING_RATE = 16000\n","\n","# El factor a multiplicar por el ruido, es acorde a:\n","#   muestra_de_ruido = muestreo + ruido * prop * escala\n","#      donde prop = amplitud_de_muestreo / amplitud_de_ruido\n","SCALE = 0.5\n","\n","BATCH_SIZE = 150\n","#EPOCHS = 100\n","EPOCHS = 50"]},{"cell_type":"code","execution_count":null,"id":"a9142aa2","metadata":{"id":"a9142aa2"},"outputs":[],"source":["# Si la carpeta 'audio' no existe, la creo; caso contrario, no hago nada.\n","if os.path.exists(DATASET_AUDIO_PATH) is False:\n","    os.makedirs(DATASET_AUDIO_PATH)\n","\n","# si la carpeta 'noise' no existe, la creo; caso contrario, no hago nada.\n","if os.path.exists(DATASET_NOISE_PATH) is False:\n","    os.makedirs(DATASET_NOISE_PATH)\n","\n","for folder in os.listdir(DATASET_ROOT):\n","    if os.path.isdir(os.path.join(DATASET_ROOT, folder)):\n","        \n","        print (\"folder = {}\".format(folder))\n","        \n","        if folder in [AUDIO_SUBFOLDER, NOISE_SUBFOLDER]:\n","            # Si la carpeta es 'audio' o 'noise', no hago nada\n","            continue\n","        elif folder in [\"other\", \"_background_noise_\"]:\n","            # Si la sub-carpeta es una de las que contienen muestras de ruido,\n","            # moverla a la carpeta 'noise'\n","            shutil.move(\n","                os.path.join(DATASET_ROOT, folder),\n","                os.path.join(DATASET_NOISE_PATH, folder),\n","            )\n","        else:\n","            # De otra forma, debe ser una carpeta de un hablante, asique hay que moverla a la carpeta de 'audio'\n","            shutil.move(\n","                os.path.join(DATASET_ROOT, folder),\n","                os.path.join(DATASET_AUDIO_PATH, folder),\n","            )"]},{"cell_type":"code","execution_count":null,"id":"1b162591","metadata":{"id":"1b162591"},"outputs":[],"source":["# Obtengo la lista de todos los archivos de ruido\n","noise_paths = []\n","for subdir in os.listdir(DATASET_NOISE_PATH):\n","    subdir_path = Path(DATASET_NOISE_PATH) / subdir\n","\n","    print(\"subdir_path= {}\".format(subdir_path))\n","\n","    if os.path.isdir(subdir_path):\n","        noise_paths += [\n","            os.path.join(subdir_path, filepath)\n","            for filepath in os.listdir(subdir_path)\n","            if filepath.endswith(\".wav\")\n","        ]\n","\n","print(\n","    \"En el directorio {}, se encontró {} archivos, pertenecientes a {} directorios\".format(\n","        DATASET_NOISE_PATH, len(noise_paths), len(os.listdir(DATASET_NOISE_PATH))\n","    )\n",")\n","\n","print(noise_paths)"]},{"cell_type":"code","execution_count":null,"id":"33a03075","metadata":{"id":"33a03075"},"outputs":[],"source":["command = (\n","    \"for dir in `ls -1 \" + DATASET_NOISE_PATH + \"`; do \"\n","    \"for file in `ls -1 \" + DATASET_NOISE_PATH + \"/$dir/*.wav`; do \"\n","    \"sample_rate=`ffprobe -hide_banner -loglevel panic -show_streams \"\n","    \"$file | grep sample_rate | cut -f2 -d=`; \"\n","    \"if [ $sample_rate -ne 16000 ]; then \"\n","    \"ffmpeg -hide_banner -loglevel panic -y \"\n","    \"-i $file -ar 16000 temp.wav; \"\n","    \"mv temp.wav $file; \"\n","    \"fi; done; done\"\n",")\n","os.system(command)\n","\n","# Dividir el ruido en fragmentos de 16000 Hz cada uno\n","def load_noise_sample(path):\n","    \n","    print(path)\n","    \n","    sample, sampling_rate = tf.audio.decode_wav(\n","        tf.io.read_file(path), desired_channels=1\n","    )\n","    if sampling_rate == SAMPLING_RATE:\n","        # Número de cortes a 16000 cada uno, que se pueden generar a partir de la muestra de ruido\n","        slices = int(sample.shape[0] / SAMPLING_RATE)\n","        sample = tf.split(sample[: slices * SAMPLING_RATE], slices)\n","        return sample\n","    else:\n","        print(\"La tasa de muestrea para {} es incorrecta. Se ignora\".format(path))\n","        return None\n","\n","\n","noises = []\n","for path in noise_paths:\n","    sample = load_noise_sample(path)\n","    if sample:\n","        noises.extend(sample)\n","noises = tf.stack(noises)\n","\n","print( \"format(len(noise_paths)=           {}\".format(len(noise_paths)) )\n","print( \"noises.shape[0]=                   {}\".format(noises.shape[0]) )\n","print( \"noises.shape[1]=                   {}\".format(noises.shape[1]) )\n","print( \"noises.shape[1] // SAMPLING_RATE = {}\".format(noises.shape[1] // SAMPLING_RATE) )\n","\n","print(\n","    \"{} archivos de ruido fueron divididos en {} muestras de ruido, donde cada una tiene {} seg. de tiempo.\".format(\n","        len(noise_paths),noises.shape[0],noises.shape[1] // SAMPLING_RATE)\n",")"]},{"cell_type":"code","source":["# Funciones para procesar Audio\n","\n","def paths_and_labels_to_dataset(audio_paths, labels):\n","    \"\"\"Construir un dataset de audios y etiquetas.\"\"\"\n","    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n","    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n","    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n","    return tf.data.Dataset.zip((audio_ds, label_ds))\n","\n","\n","def path_to_audio(path):\n","    \"\"\"Leer y decodificar un archivo de audio.\"\"\"\n","    audio = tf.io.read_file(path)\n","    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n","    return audio\n","\n","\n","def add_noise(audio, noises=None, scale=0.5):\n","    if noises is not None:\n","        # Crear un tensor aleatorio del mismo tamaño que el audio que va \n","        # desde 0 hasta la cantidad de muestras de flujo de ruido que tenemos.\n","        tf_rnd = tf.random.uniform(\n","            (tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32\n","        )\n","        noise = tf.gather(noises, tf_rnd, axis=0)\n","\n","        # Obtener la proporción de amplitud entre el audio y el ruido.\n","        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n","        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n","\n","        # Agregar el ruido reescalado al audio\n","        audio = audio + noise * prop * scale\n","\n","    return audio\n","\n","\n","def audio_to_fft(audio):\n","    # Dado que tf.signal.fft aplica FFT en la dimensión más interna, \n","    # debemos comprimir las dimensiones y luego expandirlas nuevamente \n","    # después de FFT\n","    audio = tf.squeeze(audio, axis=-1)\n","    fft = tf.signal.fft(\n","        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n","    )\n","    fft = tf.expand_dims(fft, axis=-1)\n","\n","    # Devuelve el valor absoluto de la primera mitad de la FFT \n","    # que representa las frecuencias positivas\n","    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n"],"metadata":{"id":"Yt-toMg5yFSR"},"id":"Yt-toMg5yFSR","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"96c1937a","metadata":{"id":"96c1937a"},"outputs":[],"source":["# Obtener las clases y sus etiquetas\n","\n","class_names = os.listdir(DATASET_AUDIO_PATH)\n","print(\"Posibles nombres de nuestras Clases: {}\".format(class_names,))\n","audio_paths = []\n","labels = []\n","class_names_process = []\n","\n","#Reemplazo la recorrida original de archivos de audio, por los nombres de hablantes que están en un excel.\n","#df = pd.read_excel(\"/home/jmmiguez/proyectoAudio/audio/DiputadosDebatenElAcuerdoConElFMI3.xls\")\n","#df = pd.read_excel(\"/home/jmmiguez/proyectoAudio/audio/prueba_004.xls\")\n","df = pd.read_excel(\"/content/drive/MyDrive/proyectoAudio/audio/prueba_011.xls\")\n","\n","count = df.shape[0]\n","\n","posicion = 0\n","\n","for label, name in enumerate(class_names):\n","#    print(\"Name={}- Label={}\".format(name,label,))\n","\n","    for index, row in df.iterrows():        \n","        #print(\"==>son iguales nombre={} y nombre={} +---- index{}\".format(row[2],name,index))\n","\n","        if (row[2]==name):\n","            print(\"Procesando al hablante {}\".format(row[2],))\n","            dir_path = Path(DATASET_AUDIO_PATH) / row[2]    \n","            speaker_sample_paths = [\n","                os.path.join(dir_path, filepath)\n","                for filepath in os.listdir(dir_path)\n","                if filepath.endswith(\".wav\")\n","            ]\n","            audio_paths += speaker_sample_paths\n","#            labels += [label] * len(speaker_sample_paths)\n","            labels += [posicion] * len(speaker_sample_paths)\n","            posicion = posicion + 1 \n","#        else: \n","#            print(\"No debo procesar al hablante {}\".format(row[2],))\n","            class_names_process += [name] \n","\n","        if index == count - 1:\n","            break\n","\n","#for label, name in enumerate(class_names):\n","#    print(\"Procesando al hablante {}\".format(name,))\n","#    dir_path = Path(DATASET_AUDIO_PATH) / name\n","#    speaker_sample_paths = [\n","#        os.path.join(dir_path, filepath)\n","#        for filepath in os.listdir(dir_path)\n","#        if filepath.endswith(\".wav\")\n","#    ]\n","#    audio_paths += speaker_sample_paths\n","#    labels += [label] * len(speaker_sample_paths)\n","\n","print(\"Nombres de nuestras Clases (class_names_process): {}\".format(class_names_process,))\n","\n","print(\n","    #\"Encontrados {} archivos, pertenecientes a {} clases. \\nlabels={}\".format(len(audio_paths), len(class_names), labels)\n","    \"Encontrados {} archivos, pertenecientes a {} clases. \\nlabels={} \\naudio_paths={}\".format(len(audio_paths), len(class_names_process), labels, audio_paths)\n",")\n"]},{"cell_type":"code","source":["# Obtener TRAIN_DS y VALID_DS\n","\n","# Sobreescribo: Porcentaje de muestras que voy a usar para validación\n","# originalmente, usaba VALID_SPLIT = 0.1\n","#VALID_SPLIT = 0.5\n","\n","# Mezclado\n","rng = np.random.RandomState(SHUFFLE_SEED)\n","rng.shuffle(audio_paths)\n","rng = np.random.RandomState(SHUFFLE_SEED)\n","rng.shuffle(labels)\n","\n","# Dividir entre entrenamiento y validación\n","num_val_samples = int(VALID_SPLIT * len(audio_paths))\n","print(\"Usando {} archivos para entrenamiento.\".format(len(audio_paths) - num_val_samples))\n","train_audio_paths = audio_paths[:-num_val_samples]\n","train_labels = labels[:-num_val_samples]\n","\n","print(\"Usando {} archivos para validación.\".format(num_val_samples))\n","valid_audio_paths = audio_paths[-num_val_samples:]\n","valid_labels = labels[-num_val_samples:]\n","\n","# Crear 2 datasets, uno para entrenamiento y otro para validación\n","train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n","train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n","    BATCH_SIZE\n",")\n","\n","valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n","\n","#20221113 - prueba mismo shape... para poder usar más métricas.\n","#valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n","valid_ds = valid_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n","    BATCH_SIZE\n",")\n","\n","# Agrego el ruido al conjunto de entrenamiento\n","train_ds = train_ds.map(\n","    lambda x, y: (add_noise(x, noises, scale=SCALE), y),\n","    num_parallel_calls=tf.data.AUTOTUNE,\n",")\n","\n","# Transformo las señales de audio a la frecuencia de dominio, usando 'audio_to_fft'\n","train_ds = train_ds.map(\n","    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",")\n","train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n","\n","valid_ds = valid_ds.map(\n","    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.AUTOTUNE\n",")\n","valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n"],"metadata":{"id":"UMklWwZPygo9"},"id":"UMklWwZPygo9","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4b62ca23","metadata":{"scrolled":false,"id":"4b62ca23"},"outputs":[],"source":["# Construir el modelo: layers, activación, optimizador, loss, métricas, callback con early_stopping y checkpoint\n","\n","def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n","    # Atajo\n","    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n","    for i in range(conv_num - 1):\n","        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n","        x = keras.layers.Activation(activation)(x)\n","    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n","    x = keras.layers.Add()([x, s])\n","    x = keras.layers.Activation(activation)(x)\n","    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n","\n","\n","def build_model(input_shape, num_classes):\n","    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n","\n","    x = residual_block(inputs, 16, 2)\n","    x = residual_block(x, 32, 2)\n","    x = residual_block(x, 64, 3)\n","    x = residual_block(x, 128, 3)\n","    x = residual_block(x, 128, 3)\n","\n","    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n","    x = keras.layers.Flatten()(x)\n","    x = keras.layers.Dense(256, activation=\"relu\")(x)\n","    x = keras.layers.Dense(128, activation=\"relu\")(x)\n","\n","    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n","\n","    return keras.models.Model(inputs=inputs, outputs=outputs)\n","\n","\n","#model = build_model((SAMPLING_RATE // 2, 1), len(class_names))\n","model = build_model((SAMPLING_RATE // 2, 1), len(class_names_process))\n","\n","model.summary()\n","\n","#import keras_metrics as km\n","#from keras import metrics\n","\n","model.compile(\n","    optimizer=\"Adam\", \n","    loss=\"sparse_categorical_crossentropy\", \n","    metrics=[\"accuracy\"]\n",")\n","\n","# Agregar devoluciones de llamada:\n","# 'EarlyStopping' para dejar de entrenar cuando el modelo ya no mejora.\n","# 'ModelCheckPoint' mantener siempre el modelo que tiene el mejor val_accuracy\n","model_save_filename = \"model.h5\"\n","\n","earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n","mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(model_save_filename, monitor=\"val_accuracy\", save_best_only=True)\n"]},{"cell_type":"code","execution_count":null,"id":"a31a320e","metadata":{"id":"a31a320e"},"outputs":[],"source":["# Entrenar modelo\n","\n","#sobreescribo EPOCHS = 1\n","#EPOCHS = 3\n","#EPOCHS = 50\n","\n","history = model.fit(\n","    train_ds,\n","    epochs=EPOCHS,\n","    validation_data=valid_ds,\n","    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",")"]},{"cell_type":"code","execution_count":null,"id":"71ecee95","metadata":{"id":"71ecee95"},"outputs":[],"source":["print(model.evaluate(valid_ds))"]},{"cell_type":"code","source":["#import matplotlib.pyplot as plt\n","\n","def plot_history(history):\n","    acc = history.history[\"accuracy\"]\n","    loss = history.history[\"loss\"]\n","    val_loss = history.history[\"val_loss\"]\n","    val_accuracy = history.history[\"val_accuracy\"]\n","    \n","    x = range(1, len(acc) + 1)\n","    \n","    plt.figure(figsize=(12,5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, \"b\", label=\"traning_acc\")\n","    plt.plot(x, val_accuracy, \"r\", label=\"traning_acc\")\n","    plt.title(\"Accuracy\")\n","    \n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, \"b\", label=\"traning_acc\")\n","    plt.plot(x, val_loss, \"r\", label=\"traning_acc\")\n","    plt.title(\"Loss\")\n","  \n","plot_history(history)"],"metadata":{"id":"SLL3Xp26KFUk"},"id":"SLL3Xp26KFUk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Matriz de Confusión\n","# Create x and y tensors\n","\n","x_valid = None\n","y_valid = None\n","\n","for x, y in iter(valid_ds):\n","    if x_valid is None:\n","        x_valid = x.numpy()\n","        y_valid = y.numpy()\n","    else:\n","        x_valid = np.concatenate((x_valid, x.numpy()), axis=0)\n","        y_valid = np.concatenate((y_valid, y.numpy()), axis=0)\n","\n","# Generate predictions\n","y_pred = model.predict(x_valid)\n","\n","# Calculate confusion matrix\n","confusion_mtx = tf.math.confusion_matrix(\n","    y_valid, np.argmax(y_pred, axis=-1)\n",")\n","\n","# Dibujar la matriz de confusión\n","plt.figure(figsize=(14, 5))\n","sns.heatmap(\n","    confusion_mtx, xticklabels=class_names_process, yticklabels=class_names_process, annot=True, fmt=\"g\"\n",")\n","plt.xlabel(\"Prediction\")\n","plt.ylabel(\"Label\")\n","plt.title(\"Validation Confusion Matrix\")\n","plt.show()\n","\n","for i, label in enumerate(class_names_process):\n","  if i < confusion_mtx.shape[0]:\n","    precision = confusion_mtx[i, i] / np.sum(confusion_mtx[:, i])\n","    recall = confusion_mtx[i, i] / np.sum(confusion_mtx[i, :])\n","    print(\"{0:15} Precision:{1:.2f}%; Recall:{2:.2f}%\".format(label, precision * 100, recall * 100))\n","    "],"metadata":{"id":"xteoENceYIdF"},"id":"xteoENceYIdF","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4bfe19f1","metadata":{"id":"4bfe19f1"},"outputs":[],"source":["\n","\n","\n","#redefino BATCH_SIZE, por si la cantidad de muestras es menor a 128\n","BATCH_SIZE = 128\n","SAMPLES_TO_DISPLAY = 10\n","\n","print(\"valid_audio_paths={} \\n valid_labels={}\".format(valid_audio_paths, valid_labels))\n","\n","test_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n","\n","\n","#print(\"test_ds={}\".format(test_ds[-1]))\n","test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(BATCH_SIZE)\n","test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n","\n","for audios, labels in test_ds.take(1):\n","    # Obtener la señal FFT\n","    ffts = audio_to_fft(audios)\n","    # Predecir\n","    y_pred = model.predict(ffts)\n","    # Tomar muestras aleatorias\n","    rnd = np.random.randint(0, BATCH_SIZE, SAMPLES_TO_DISPLAY)\n","    \n","   \n","    audios = audios.numpy()[rnd, :, :]\n","    labels = labels.numpy()[rnd]\n","    y_pred = np.argmax(y_pred, axis=-1)[rnd]\n","\n","\n","\n","    print(\"labels= {}, y_pred={}\".format(labels,y_pred))\n","\n","    cm=confusion_matrix(labels, y_pred)\n","    print(cm)\n","    \n","    for index in range(SAMPLES_TO_DISPLAY):\n","        # Para cada muestra, imprimir la etiqueta verdadera y predicha, así como ejecutar la voz con su ruido\n","        print(\n","            \"Speaker:\\33{} {}\\33[0m\\tPredicted:\\33{} {}\\33[0m\".format(\n","                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n","                class_names_process[labels[index]],\n","                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n","                class_names_process[y_pred[index]],\n","            )\n","        )\n","        display(Audio(audios[index, :, :].squeeze(), rate=SAMPLING_RATE))"]},{"cell_type":"code","execution_count":null,"id":"82d4da52","metadata":{"id":"82d4da52"},"outputs":[],"source":["#------------------------------------------------------------------------------------------------------------------------------"]},{"cell_type":"code","execution_count":null,"id":"28d8b2ca","metadata":{"id":"28d8b2ca"},"outputs":[],"source":["# Clasificar a qué categoría corresponde el audio VozViva.wav\n","\n","BATCH_SIZE = 1\n","#SAMPLES_TO_DISPLAY = 1\n","\n","valid_audio_paths_vivo = []\n","valid_audio_paths_vivo += [\"{}\".format(DATASET_ROOT + '/audio/VozViva/VozViva.wav')]\n","valid_labels += []\n","test_ds = paths_and_labels_to_dataset(valid_audio_paths_vivo, [6])\n","test_ds = test_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(BATCH_SIZE)\n","test_ds = test_ds.map(lambda x, y: (add_noise(x, noises, scale=SCALE), y))\n","for audios, labels in test_ds.take(1):\n","    # Obtener la señal FFT\n","    ffts = audio_to_fft(audios)\n","    # Predecir\n","    y_pred = model.predict(ffts)\n","    max_pred_value=0\n","    idx_max_pred_value=0\n","    name_max_pred_value=\"\"\n","    pred_name = class_names_process[np.argmax(y_pred)]\n","    print(pred_name)\n","    print(\"0)  -- {} {} \".format(pred_name, y_pred) )\n","    for index, value in enumerate(y_pred[0]):\n","        if (value > 0.1):\n","          print(\"y_pred({})<={} {}\".format(index,value, class_names_process[index]) )\n","        if(value>max_pred_value):\n","            max_pred_value=value\n","            idx_max_pred_value=index\n","            name_max_pred_value=pred_name\n","    print(\"1) proba.cercanía=[{}] orden_categoría={} nombre_categoría={}\".format(max_pred_value, idx_max_pred_value, name_max_pred_value) )\n","    y_pred = np.argmax(y_pred, axis=-1)\n","    print(\"2) predicción(Voz en Vivo) = {} ... y_pred={} index={}\".format(class_names_process[y_pred[0]], y_pred, index))\n","    print(\"3) registro (Voz en Vivo) = {} - {}\".format(valid_audio_paths_vivo[0], y_pred,  ) )\n","\n","    display(Audio( valid_audio_paths_vivo[0], rate=SAMPLING_RATE ) )    "]},{"cell_type":"code","execution_count":null,"id":"a2bb561d","metadata":{"id":"a2bb561d"},"outputs":[],"source":["# Dibujar forma de onda y espectrograma del audio: VozViva.wav\n","\n","# Read the wav file (mono)\n","samplingFrequency, signalData = wavfile.read('/content/drive/MyDrive/proyectoAudio/audio/audio/VozViva/VozViva.wav')\n","\n","# Plot the signal read from wav file\n","plt.rcParams[\"figure.figsize\"] = [10.00, 8.00]\n","plt.rcParams[\"figure.autolayout\"] = True\n","plt.subplot(211)\n","plt.title('Wave')\n","plt.plot(signalData)\n","plt.xlabel('Sample')\n","plt.ylabel('Amplitude')\n","plt.subplot(212)\n","plt.title('Spectrogram')\n","plt.specgram(signalData,Fs=samplingFrequency)\n","plt.xlabel('Time')\n","plt.ylabel('Frequency')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"1VSzJQ7gYwx3lT57zcpaD2Kfn8iwaYk5J","timestamp":1669590215154},{"file_id":"14lwd9LDnZZJxBSMUNttqzsyPLi-aUJSt","timestamp":1668472593533},{"file_id":"1Ycpg72aklXOpqr1gC7aVZY8_TPnttU5f","timestamp":1668453118727},{"file_id":"15GGbldPtE20VCG2rt2-pnV-IA2CM8M8M","timestamp":1668434423631},{"file_id":"1xVPd7Zd-FnGF89b_SaNUtdXcuC09awIU","timestamp":1668382578669},{"file_id":"1N1_4PMhURq2p7PGdyd0cTrWX4UM01IL6","timestamp":1668033310885},{"file_id":"1GPzvbeKyJnp4FAD8B8MMj_fyckvi1xtx","timestamp":1662133294352}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}